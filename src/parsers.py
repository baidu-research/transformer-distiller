import argparse
import json
import os
import shutil
import yaml
from .callbacks import callbacks
from .utils import OPT_CLASS_DICT, LR_SCHEDULER_DICT
from .textbrewer import TEMPERATURE_SCHEDULER, WEIGHT_SCHEDULER, KD_LOSS_MAP
import sys
root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(root)
from methods import METHODS


# All distillation algorithm related parameters
distillation_parser = argparse.ArgumentParser(
                            formatter_class=argparse.RawTextHelpFormatter,
                            add_help=False)

# teacher and student model
model_group = distillation_parser.add_argument_group('model')
model_group.add_argument('teacher', help='a model name string in '
                         'huggingface, e.g., bert-large-cased, or path to '
                         'a dir that contains model config file and a'
                         'binary checkpoint')
model_group.add_argument('student', help='could be: 1) path to a student '
                         'config file. Then a student will be initialized '
                         'from scratch. Or 2) path to a dir that contains '
                         'student config and binary checkpoint. In this '
                         'case, the student checkpoint will be loaded')

# data
data_group = distillation_parser.add_argument_group('data')
data_group.add_argument('--data', required=True, nargs='+',
                        help='path to one or multiple binary data files. '
                        'The binary file is generated by calling '
                        'python src/data/scripts/h5_data.py with the '
                        'tokenizer corresponding to the teacher model.')
data_group.add_argument('--eval-data', nargs='+',
                        help='same as data, but for evaluting loss')
data_group.add_argument('--shuffle', type=bool, default=True,
                        help='whether to shuffle data indices to create batch')
data_group.add_argument('--token-counts', type=str, nargs='+',
                        help='path to one or multiple token counts binary '
                        'files. Only useful if using masked language model '
                        'head to produce logits')
data_group.add_argument('--mlm-pred-rate', type=float, default=0.15,
                        help='fraction of tokens to be predicted in masked '
                        'language modeling loss')
data_group.add_argument('--mlm-mask-rate', type=float, default=0.8,
                        help='fraction of masking at to-be-predicted tokens')
data_group.add_argument('--mlm-rand-rate', type=float, default=0.1,
                        help='fraction of replacing by a random token at '
                        'to-be-predicted tokens')
data_group.add_argument('--mlm-keep-rate', type=float, default=0.1,
                        help='fraction of keeping the original token at '
                        'to-be-predicted tokens. It is assumed that '
                        'mlm-mask-rate + mlm-rand-rate + mlm-keep-rate = 1.0')
data_group.add_argument('--lm-type', choices=['mlm', 'clm'],
                        help='The type of LM head that produces logits. '
                        'Usually we use mlm for distilling bert, roberta; and '
                        'clm for distilling gpt2')

# Distillation choices, loss weight, intermediate layer alignment, etc
# The names are consistent with ../textbrewer/configurations.py: DistillationConfig
distill_group = distillation_parser.add_argument_group('distillation')
distill_group.add_argument('--temperature', type=float, default=1.0,
                           help='temperature for cross-entropy loss')
distill_group.add_argument('--temperature-scheduler', default='none',
                           choices=list(TEMPERATURE_SCHEDULER.store.keys()).append('none'),
                           help='temperature scheduler')
distill_group.add_argument('--hard-label-weight', type=float, default=0.,
                           help='hard label weight')
distill_group.add_argument('--hard-label-weight-scheduler', default='none',
                           choices=list(WEIGHT_SCHEDULER.keys()).append('none'),
                           help='hard label weight scheduler')
distill_group.add_argument('--kd-loss-type', default='ce',
                           choices=list(KD_LOSS_MAP.keys()),
                           help='KD loss type')
distill_group.add_argument('--kd-loss-weight', type=float, default=1.0,
                           help='KD loss weight')
distill_group.add_argument('--kd-loss-weight-scheduler', default='none',
                           choices=list(WEIGHT_SCHEDULER.keys()).append('none'),
                           help='KD loss weight scheduler')
distill_group.add_argument('--probability-shift', action='store_true',
                           help='switch the ground-truth label\'s logit and '
                           'the largest logit predicted by the teacher. '
                           'Requires `labels` term returned by adaptor')
distill_group.add_argument('--intermediate-matches',
                           help='path to a json list file. Loading the file '
                           'returns a list of dictionaries, where each '
                           'dictionary specifies one layer-layer alignment '
                           'between teacher and student.\n'
                           'The dictionary has the following keys:\n'
                           'layer_T: indices of teacher\'s transformer layer, '
                           'starting from 1. Could be a single int index, or '
                           'a list of int indices\n'
                           'layer_S: indices of student\'s transformer layer, '
                           'that is going to match `layer_T`. Same format as '
                           'layer_T\n'
                           'feature: feature used for the alignment. Choose'
                           'one of {"hidden", "attention"}\n'
                           'loss: choose one key from src.MATCH_LOSS_MAP\n'
                           'weight (float): weight for this loss\n'
                           'proj: type of projection applied on student\'s '
                           'feature. A list of the following format, '
                           '[proj_type, student_dim, teacher_dim], where '
                           'proj_type is one from the keys of src.PROJ_MAP. '
                           'teacher_dim, student_dim are the dimensions of '
                           'teacher or student feature respectively')
distill_group.add_argument('--is-caching-logits', action='store_true',
                           help='cache teacher logit the first pass, saving '
                           'training time')

# optimization choices
opt_group = distillation_parser.add_argument_group('optimizer')
opt_group.add_argument('--opt-type', default='adam_fused',
                       choices=list(OPT_CLASS_DICT.keys()),
                       help='optimizer name. Choose from one of the keys '
                       'defined in opt_utils.py')
opt_group.add_argument('--fp16', action='store_true',
                       help='Use apex.amp optimzier if specified.')
opt_group.add_argument('--fp16-opt-level', choices=['O0', 'O1', 'O2', 'O3'],
                       help='Apex.amp opt_level')
opt_group.add_argument('--max-steps', type=int, default=400000,
                       help='max number of training steps')
opt_group.add_argument('--max-grad-norm', type=float, default=-1,
                       help='clip gradient if its norm exceeds this value. '
                       'Default to -1, not clipping.')
opt_group.add_argument('--bsz', type=int, default=256,
                       help='batch size')
opt_group.add_argument('--grad-acc-step', type=int, default=1,
                       help='If the batch size causes out-of-memory. Set '
                       'this value to be bigger than 1, and divisible by '
                       'local batch size, i.e., bsz/num_ranks')
opt_group.add_argument('--weight-decay', type=float, default=0.,
                       help='weight decay')
opt_group.add_argument('--lr', type=float, default=6e-4,
                       help='max learning rate')
opt_group.add_argument('--lr-scheduler', default='constant',
                       choices=list(LR_SCHEDULER_DICT.keys()),
                       help='choice of learning rate scheduler. One from the '
                       'keys defined in opt_utils.py')
opt_group.add_argument('--warmup-steps', type=int, default=0,
                       help='warm up steps (default to 0)')
opt_group.add_argument('--resume-from',
                       help='path to a state dict to resume training')

# logging, eval and checkpoint choices
hkeep_group = distillation_parser.add_argument_group('housekeeping')
hkeep_group.add_argument('--output-dir', required=True,
                         help='dir to save all necessary things')
hkeep_group.add_argument('--log-freq-iter', type=int, default=10,
                         help='report loss every so many training iterations')
hkeep_group.add_argument('--ckpt-freq-iter', type=int, default=10000,
                         help='save and run callback(s) on student model '
                         'every so many training iterations')
hkeep_group.add_argument('--save-ini', action='store_true',
                         help='save the student before any training. '
                         'Useful for studying random initialization')
opt_group.add_argument('--callback', nargs='+',
                       choices=callbacks.all_callbacks,
                       help='callback function invoked at checkpointing.')
opt_group.add_argument('--glue-task', nargs='+', default=['mnli'],
                       choices=['cola', 'mnli', 'mrpc', 'qnli', 'qqp',
                                'rte', 'sst2', 'stsb', 'wnli'],
                       help='one or multiple glue task to run as callbacks. '
                       'Only useful if --callback specifies glue')

# misc, reproducibility, etc.
misc_group = distillation_parser.add_argument_group('misc')
misc_group.add_argument('--seed', type=int, default=0,
                        help='random seed that controls all operations that '
                        'involves randomness')


def parse_job_config(job_yaml):
    """
    Parse the job_yaml into a string of the followings:

    distillation_config_str: ` positional_param1 positional_param2 --optional1 val --optional2 val2 ...`
    sbatch_config_str: `sbatch --nodes $nodes --gres=gpu:$ntasks_per_node ...`
    
    Note: User is supposed to make sure fields in his/her job yaml file 
    matches the arguments in distillation_parser
    """
    with open(job_yaml, 'r') as f:
        cfg = yaml.safe_load(f)

    # create output dir
    if not os.path.exists(cfg['output-dir']):
        os.makedirs(cfg['output-dir'])

    # copy job config file into the output dir
    if not os.path.exists(os.path.join(cfg['output-dir'], 'job_config.yaml')):
        shutil.copyfile(job_yaml,
                        os.path.join(cfg['output-dir'], 'job_config.yaml'))

    # distillation main method
    method = cfg['method']
    py_program = METHODS[method]

    # positional args for py program
    distillation_config_str = '{} {}'.format(cfg['teacher'], cfg['student']) 

    # sbatch related parameters
    sbatch_config_str = ''

    for key, val in cfg.items():
        if key in ['teacher', 'student', 'method']:
            continue

        if key == 'intermediate-matches': # avoid wrapping json string
            match_json = os.path.join(cfg['output-dir'], 'matches.json') 
            with open(match_json, 'w') as f:
                f.write(json.dumps(val))
            distillation_config_str += ' --intermediate-matches {}'.format(match_json)
        elif key in ['fp16', 'save-ini']: # switchs
            if val:
                distillation_config_str += ' --'+key
        elif key == 'sbatch':
            for sub_key, sub_val in val.items():
                sbatch_config_str += ' --{}={}'.format(sub_key, sub_val)
        else: # most of them are (arg, value) pairs for the python program
            if isinstance(val, list): # if value is a list
                val = ' '.join([str(_) for _ in val]) 
            distillation_config_str += ' --{} {}'.format(key, val)

    # a few additional parameters for sbatch
    sbatch_config_str += ' --gres=gpu:{}'.format(cfg['sbatch']['ntasks-per-node'])
    sbatch_config_str += ' --signal=USR1@60' # send signal 60s before time limit

    # The actual bash command that submit the job
    submisson_cmd = 'sbatch' + sbatch_config_str + \
        ' --wrap "srun python {} {}"'.format(py_program, distillation_config_str)

    # The job resubmission bash command
    # `restore.ckpt` is used for ckpt at time limit
    resume_ckpt_path = os.path.join(cfg['output-dir'], 'restore.ckpt') 
    if '--resume-from' not in distillation_config_str:
        distillation_config_str_for_resume = distillation_config_str + \
            " --resume-from " + resume_ckpt_path
    else:
        all_fields = distillation_config_str.split()
        all_fields[all_fields.index('--resume-from') + 1] = resume_ckpt_path
        distillation_config_str_for_resume = ' '.join(all_fields)
    resume_cmd = 'sbatch' + sbatch_config_str + \
        ' --wrap "srun python {} {}"'.format(py_program, distillation_config_str_for_resume)

    # write the commands
    with open(os.path.join(cfg['output-dir'], 'submission.sh'), 'w') as f:
        f.write(submisson_cmd)
    with open(os.path.join(cfg['output-dir'], 'resume.sh'), 'w') as f:
        f.write(resume_cmd)
    return submisson_cmd, resume_cmd
