# use absolute path throughout
# required args
method: minilm2 
teacher: roberta-large 
student: exp/minilm2/roberta_d384_h12_l6.json 
data:
  - $PATH_TO_WIKI_TOKENIZED_WITH_ROBERTA/data.h5
  - $PATH_TO_BOOK_TOKENIZED_WITH_ROBERTA/data.h5

eval-data:
  - $PATH_TO_MNLI_TOKENIZED_WITH_ROBERTA/data.h5

# Distillation algorithm related args
teacher-layers: 19 
student-layers: 6
num-relation-heads: 64

# optimization related args
opt-type: adamw
fp16: True
fp16-opt-level: O1
max-steps: 500000
bsz: 256 
#weight-decay: 1e-2
grad-acc-step: 1
lr: 1e-3
lr-scheduler: constant
#warmup-steps: 1000

# log, ckpt etc
output-dir: results/minilm2/roberta-large
ckpt-freq-iter: 50000
log-freq-iter: 10
save-ini: True

# callbacks
callback:
  - glue
glue-task:
  - mnli
  - qqp

# sbatch configs
# Use comma to separate values if a list
sbatch:
    partition: V100x8
    nodes: 1 
    ntasks-per-node: 8
    cpus-per-task: 5 
    output: results/minilm2/roberta-large
